[{"role": "system", "content": "You are an expert researcher and strategic planner with a deep understanding of experimental design and reproducibility in scientific research. \nYou will receive a research paper in JSON format. \nYour task is to create a detailed and efficient plan to reproduce the experiments and methodologies described in the paper.\nThis plan should align precisely with the paper's methodology, experimental setup, and evaluation metrics. \n\nInstructions:\n\n1. Align with the Paper: Your plan must strictly follow the methods, datasets, model configurations, hyperparameters, and experimental setups described in the paper.\n2. Be Clear and Structured: Present the plan in a well-organized and easy-to-follow format, breaking it down into actionable steps.\n3. Prioritize Efficiency: Optimize the plan for clarity and practical implementation while ensuring fidelity to the original experiments."}, {"role": "user", "content": "## Paper\n{'paper_id': 'crypto_sentiment_paper', 'title': 'Revisiting Financial Sentiment Analysis: A Language Model Approach', 'abstract': 'Financial Sentiment Analysis (FSA) traditionally relies on human-annotated sentiment labels to infer investor sentiment and forecast market movements. However, inferring the potential market impact of words based on human-perceived intentions is inherently challenging. We hypothesize that the historical market reactions to words, offer a more reliable indicator of their potential impact on markets than subjective sentiment interpretations by human annotators. To test this hypothesis, a market-derived labeling approach is proposed to assign tweet labels based on ensuing short-term price trends, enabling the language model to capture the relationship between textual signals and market dynamics directly. A domain-specific language model was fine-tuned on these labels, achieving up to an 11% improvement in short-term trend prediction accuracy over traditional sentiment-based benchmarks. Moreover, by incorporating market and temporal context through prompt-tuning, the proposed context-aware language model demonstrated an accuracy of 89.6% on a curated dataset of 227 impactful Bitcoinrelated news events with significant market impacts. Aggregating daily tweet predictions into trading signals, our method outperformed traditional fusion models (which combine sentiment-based and price-based predictions). It challenged the assumption that sentiment-based signals are inferior to price-based predictions in forecasting market movements. Backtesting these signals across three distinct market regimes yielded robust Sharpe ratios of up to 5.07 in trending markets and 3.73 in neutral markets. Our findings demonstrate that language models can serve as effective short-term market predictors. This paradigm shift underscores the untapped capabilities of language models in financial decision-making and opens new avenues for market prediction applications.', 'pdf_parse': {'paper_id': 'crypto_sentiment_paper', 'abstract': [{'text': 'Financial Sentiment Analysis (FSA) traditionally relies on human-annotated sentiment labels to infer investor sentiment and forecast market movements. However, inferring the potential market impact of words based on human-perceived intentions is inherently challenging. We hypothesize that the historical market reactions to words, offer a more reliable indicator of their potential impact on markets than subjective sentiment interpretations by human annotators. To test this hypothesis, a market-derived labeling approach is proposed to assign tweet labels based on ensuing short-term price trends, enabling the language model to capture the relationship between textual signals and market dynamics directly. A domain-specific language model was fine-tuned on these labels, achieving up to an 11% improvement in short-term trend prediction accuracy over traditional sentiment-based benchmarks. Moreover, by incorporating market and temporal context through prompt-tuning, the proposed context-aware language model demonstrated an accuracy of 89.6% on a curated dataset of 227 impactful Bitcoinrelated news events with significant market impacts. Aggregating daily tweet predictions into trading signals, our method outperformed traditional fusion models (which combine sentiment-based and price-based predictions). It challenged the assumption that sentiment-based signals are inferior to price-based predictions in forecasting market movements. Backtesting these signals across three distinct market regimes yielded robust Sharpe ratios of up to 5.07 in trending markets and 3.73 in neutral markets. Our findings demonstrate that language models can serve as effective short-term market predictors. This paradigm shift underscores the untapped capabilities of language models in financial decision-making and opens new avenues for market prediction applications.', 'section': 'Abstract', 'sec_num': None}], 'body_text': [{'text': \"Today, a single social media post from a public figure can trigger billion-dollar fluctuations in global markets within minutes. As markets evolve, they are increasingly shaped by the collective psychology and behavior of investors. Investor sentiment, often reflected in social media posts on platforms such as X (formerly Twitter), serves as a proxy for market expectations [1] . FSA quantifies these sentiments, typically classified as Bullish, Bearish, or Neutral, to forecast market movements. However, significant challenges remain in translating these classifications into actionable trading insights. First, sentiment classifications often lack precision and explainability. For instance, a Bullish label might indicate anything from a modest 2% price increase over five days to a substantial 10% rise over a month, making it difficult to establish clear trading strategies based on such vague signals [2] . Second, traditional FSA relies on human-annotated sentiment labels, which embed subjective interpretations of a text's market impact. These preconceived notions often diverge from historical market reactions, introducing a gap between sentiment perception and measurable outcomes. Finally, financial tweets differ significantly from traditional textual data due to their brevity, domain-specific jargon, and references to real-time events. Ignoring this context diminishes the relevance of sentiment analysis for predicting market trends [3] . To address these limitations, we propose a novel approach to FSA that focuses on measurable market reactions rather than inferred intentions. Instead of relying on subjective, human-annotated sentiment labels, market-derived annotation is adopted, which directly ties labels to subsequent market movements. This approach leverages financial markets' well-defined metrics to replace manual annotations with precise, automated labels [2, 4] . By ground-ing sentiment in historical market responses, a closer alignment between sentiment analysis and actionable trading strategies is ensured. Additionally, contextual market data is incorporated into the analysis using a prompt engineering technique which enables language models to integrate key market indicators, such as price trends and market volatility directly into textual analysis [5] . By embedding this context into tweets, our method transforms FSA into a unified framework that synthesizes sentiment analysis and market prediction into one. Unlike traditional multi-model approaches, our framework eliminates the need for separate price prediction models and fusion layers, offering a language model approach to the problem of FSA.\", 'section': 'Introduction', 'sec_num': '1'}, {'text': 'This study introduces a new perspective on sentiment analysis in finance, demonstrating that language models can be effective short-term Bitcoin market predictors. This contrasts starkly with previous approaches where language models, relying on human-annotated sentiment or lacking market context, have shown limited predictive capability on their own. Our work thus calls to revisit the assumptions underlying traditional FSA, highlighting the untapped potential of language models for shortterm market prediction when properly fine-tuned. Specifically, our main contributions are as follows:', 'section': 'Introduction', 'sec_num': '1'}, {'text': '\u2022 Market-Driven Labeling: We replace subjective sentiment labels with market-derived labels that directly reflect ensuing price movements, enabling the model to learn from actual market behavior rather than inferred sentiment.', 'section': 'Introduction', 'sec_num': '1'}, {'text': '\u2022 Context-Enhanced Prompt Tuning: By embedding key market information into textual prompts, our approach expands the feature space of language models, providing the model with the context required for precise market predictions. This simple end-to-end language-model approach, shown in Figure 1 , offers a practical and robust solution to FSA, demonstrating the potential of such an approach by effectively unifying textual and market data within a single model.', 'section': 'Introduction', 'sec_num': '1'}, {'text': \"The remaining part of this paper is organized as follows. Section 2 provides a background to the labeling method, and language models used in the rest of the paper. Section 3 presents the related works that inspired this study. Section 4 explains the labeling process, proposed language models, prompt tuning process, proposed signal generation methods, and backtesting strategies. Section 5 presents and interprets the experimental results on tweet classification, signal generation, and backtesting of our proposed methods, along with their comparison to the benchmarks. Section 6 involves this study's limitations and implications of this study, and Section 7 concludes the study's highlights and suggests multiple future directions for the readers.\", 'section': 'Introduction', 'sec_num': '1'}, {'text': 'This section presents the challenges of market-derived labeling and Triple Barrier Labeling (TBL) [4] as the basis for our proposed solution. We also introduce CryptoBERT and FinBERT as the base domain-specific language models for the fine-tuning process.', 'section': 'Background', 'sec_num': '2'}, {'text': 'Labeling short-term market trends as Bullish, Bearish, or Neutral requires a method that is precise, adaptable, and aligned with trading applications. In this context, short-term trends represent the overall price direction of an asset over a defined timeframe (typically 8 to 15 days, as supported by studies of post behavior on X [6] ). TBL provides a robust framework for this purpose by incorporating three distinct barriers that capture market behavior dynamically:', 'section': 'Triple Barrier Labeling', 'sec_num': '2.1'}, {'text': '1. Upper Barrier: A profit-taking price threshold that, when reached, labels the trend window as Bullish.', 'section': 'Triple Barrier Labeling', 'sec_num': '2.1'}, {'text': '2. Lower Barrier: A stop-loss price threshold that, when breached, labels the trend window as Bearish.', 'section': 'Triple Barrier Labeling', 'sec_num': '2.1'}, {'text': '3. Vertical Barrier: A temporal limit that marks the expiration of an observation window, ensuring trends are assessed within a fixed timeframe.', 'section': 'Triple Barrier Labeling', 'sec_num': '2.1'}, {'text': \"Unlike traditional fixed-time horizon methods, TBL is a pathdependent labeling strategy, meaning it evaluates the entire price trajectory within the observation window. Labels are determined by the first barrier touched during this period, with dynamic adjustments based on market volatility [4] . For instance, the horizontal barriers (upper and lower) are scaled by a volatility estimate, ensuring that they are sensitive to the asset's price fluctuations. The vertical barrier ensures that observations remain time-bounded, preventing positions from being unrealistically held indefinitely. TBL addresses several key challenges in financial labeling. It provides an intuitive and explainable labeling mechanism that aligns with real-world trading practices [7] . By dynamically adjusting barriers based on market volatility, it remains robust across different market regimes. Its path dependence ensures that labels reflect the actual price trajectory rather than arbitrary points within a fixed timeframe, reducing lookahead bias. For example, in a volatile Bitcoin market, TBL adjusts the horizontal barriers to accommodate rapid price swings, ensuring that labels remain consistent with actionable trading opportunities. This adaptability makes TBL highly suitable for live trading and machine-learning applications [8] . Moreover, by combining profit-taking, stop-loss, and temporal constraints, TBL provides a comprehensive framework that captures market dynamics effectively while mitigating overfitting risks.\", 'section': 'Triple Barrier Labeling', 'sec_num': '2.1'}, {'text': 'Sentiment analysis in financial markets emerged from the growing recognition that market psychology plays a crucial role in market movements [9] . As Natural Language Processing (NLP) techniques advanced, they enabled the extraction of sentiments and emotions from large volumes of unstructured text, providing a means to quantify market psychology [10, 11] . Social media platforms, particularly Twitter, became valuable sources of real-time insights into investor sentiment, which could be used to predict financial market behavior. Sentiment analysis, or opinion mining, had already been established as a method within NLP for extracting sentiments, making them well-suited for quantifying investor psychology and applying these signals to financial markets. The power of sentiment analysis in forecasting market trends, volatility, volume, and risk contributed to the birth of FSA [3, 2] . The primary goal of FSA has been to utilize NLP methods to capture investor sentiment and psychology, providing valuable insights for predicting market movements [2] . Over time, the field has evolved, and various methods have been developed to refine sentiment classification, improve prediction accuracy, and integrate more nuanced factors such as market context. In the subsequent sections, we will explore the current state of the art in FSA, examine existing gaps, and highlight how this study aims to address these challenges.', 'section': 'Related Works', 'sec_num': '3'}, {'text': \"FSA spans a wide range of methods, including lexicon-based approaches [12] , word representations [13] , and pre-trained language models [14, 15, 16, 17] . It also draws on various data sources such as news headlines [18] , social media posts [19] , and financial documents [3] , with applications in volatility prediction, price prediction, and trend forecasting [2] . The core goal of FSA is to quantify investor sentiment from textual data to inform decision-making in financial markets. In this study, we focus on predicting Bitcoin's short-term market movements by quantifying sentiment from Bitcoin-related tweets. This task has been addressed by various studies, but we consider PreBit [20] to be the state-of-the-art in this area. Unlike many other studies, PreBit advances the field by passing the actual tweet representations, rather than their sentiment class labels, to the trend prediction model. This allows the model to learn patterns between the words in the tweets and the market movements they ought to predict. By retaining the tweet's representation, the model can identify nuanced relationships between specific words and their effect on market behavior, bridging the gap between textual sentiment and market outcomes.\", 'section': 'Financial Sentiment Analysis Scope', 'sec_num': '3.1'}, {'text': \"Despite advancements like these, FSA models are still often used in conjunction with price-based prediction models because they do not perform adequately on their [3] . This limitation is not necessarily due to the inferiority of textual data compared to price-based features. Even when domain-specific, pre-trained word embeddings are used, the issue lies in how we still assign sentimental value to words based on subjective perceptions, rather than allowing the market's reaction to define these relationships. This leads to our core hypothesis: market-driven labels, learned from actual market movements, can better inform sentiment analysis models. By letting the market itself teach the model the significance of words, we aim to bridge the gap between textual content and market reactions, moving away from subjective interpretations. Our hypothesis also points to a second major challenge: contextualization. The context in which a tweet is shared-such as market conditions, sentiment trends, or external factors-can be more influential than the content itself. Previous research supports this idea, showing that expanded feature spaces, including factors like author credibility [1, 21, 22] , post volume [23] , and extracted topics of discussion [24] , significantly improve the performance of FSA models. In the next section, we discuss the latest developments in NLP and FSA that have inspired our research and highlight how our approach builds on these advances.\", 'section': 'Financial Sentiment Analysis Scope', 'sec_num': '3.1'}, {'text': 'Large Language Models (LLMs) have increasingly been leveraged for financial market analysis, with prominent examples such as FinBERT [14] , CryptoBERT [15] , and FinEAS [17] . These models have set a strong foundation for improving FSA tasks. FinBERT, for instance, adapts to the finance domain by effectively incorporating contextual information from financial texts, outperforming traditional machine learning algorithms in sentiment classification. Similarly, FinEAS, building upon Sentence-BERT, generates high-quality sentence embeddings that substantially enhance FSA task performance. CryptoBERT, which serves as the foundation for our study, is fine-tuned on cryptocurrency-related social media data, leveraging the BERTweet architecture and domain-specific training to accurately analyze crypto-related tweets. On the other hand, FinGPT [16] represents a breakthrough as a general-purpose financial advisor, demonstrating how LLMs can synthesize market data and provide accessible insights for developing financial models. In our work, we build upon CryptoBERT by incorporating market and temporal context into tweets, enabling the model to better detect patterns and improve prediction accuracy, ultimately offering more actionable insights for cryptocurrency investors. Our study is further inspired by research that examined the temporal window of tweet propagation during events like the COVID-19 and H1N1 pandemics, which suggested that the most impactful tweet propagation occurs within a window of 8 to 15 days [6, 25] . This finding informed our decision to focus on short-term trend prediction in the market, aligning with the intuition that market psychology (driven by fear and greed) manifests strongly in short-term fluctuations.', 'section': 'Innovations in NLP, and The FSA Horizon', 'sec_num': '3.2'}, {'text': 'A key innovation in improving text classification tasks like FSA has been prompt engineering [26] . Specifically, studies such as Dhar et al. [27] have demonstrated the transformative impact of prompt engineering on financial decision-making, enhancing operational efficiency, risk management, and customer service. Ahmed et al. [5] ', 'section': 'Innovations in NLP, and The FSA Horizon', 'sec_num': '3.2'}, {'text': \"This section presents our proposed method, starting with a description of the datasets and a novel trend-based labeling approach inspired by the TBL method. We then expand the language model's feature space through prompt tuning, enabling it to capture complex interrelations among textual content, market indicators, and temporal dynamics. We introduce two trading signal generation methods to predict the direction and magnitude of trading positions. You can find all the implementation details in a public repository 1 . Finally, we introduce our proposed backtesting strategies and evaluation metrics.\", 'section': 'Proposed Method', 'sec_num': '4'}, {'text': 'We curated datasets comprising daily tweets, technical indicators, and significant Bitcoin events to support market trend prediction using sentiment and technical indicators. Each dataset serves a distinct purpose: tweets are used for model training and evaluation; technical indicators are incorporated as price-based features in classification models and as market context during the prompt-tuning process. also, a dataset of historical events is leveraged to benchmark model performance on key dates, particularly during periods of high market volatility or impactful news cycles. Here we present these datasets in more detail:', 'section': 'Data Collection', 'sec_num': '4.1'}, {'text': '\u2022 Bitcoin Historical Events: This dataset contains 227 significant Bitcoin market events from 2009 to 2024, focusing on benchmarking model performance during influential periods between 2015 and 2023. Accessible from Kaggle2 , the dataset catalogs pivotal news events such as regulatory announcements, major hacks, and institutional adoptions that historically triggered substantial price volatility. By including these events, the dataset enables performance evaluation during market conditions that often require rapid and nuanced sentiment analysis, providing insights into how well the models respond to real-world, high-stakes scenarios.', 'section': 'Data Collection', 'sec_num': '4.1'}, {'text': '\u2022 Tweets: Tweet data from two sources, spanning 2015 to early 2023, was utilized. The first dataset, covering from January 2015 to February 2021, was obtained from PreBit [20] , and the second dataset, sourced from Kaggle Together, these datasets form a multi-faceted basis for trend prediction, enhancing accuracy and generalization across varying market conditions.', 'section': 'Data Collection', 'sec_num': '4.1'}, {'text': \"Most FSA studies rely on human-annotated sentiment labels to train language models for classifying financial tweets. These sentiment classifications are then used to predict next-day market movements, typically using fixed thresholds for bullish and bearish labels based on the next day's ROC [30, 2] . However, our approach takes a fundamentally different path by directly using market-derived labels to classify tweets, bypassing the subjectivity of human annotations. While the influence of social media sentiment on financial markets often extends up to 15 days, many studies oversimplify this relationship by relying on single-day thresholds [6] . Such methods fail to capture the prolonged effects of sentiment and ignore real-world trading practices, where trends span multiple days. Effective labeling must account for these dynamics and incorporate mechanisms like take-profit, stop-loss, and vertical barriers to manage risk and adapt to market volatility [7] . To address these challenges, we propose a trend-based labeling method inspired by TBL. This approach integrates historical volatility and market-defined barriers, creating explainable and actionable labels that establish a direct connection between textual inputs and market trends. The method dynamically adjusts upper and lower barriers based on market volatility, as detailed below:\", 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': '\u2022 Historical Volatility Estimation: The historical volatility of log returns is estimated using the Exponentially Weighted Moving Average (EWMA) method. Log returns are calculated as shown in Eq. ( 1):', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': 'EQUATION', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': 'where P t represents the closing price at time t. Log returns quantify the relative change in price between consecutive time steps and serve as the basis for volatility estimation.', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': 'Using the log returns, the EWMA method calculates their exponentially weighted standard deviation. The formula for EWMA volatility is given in Eq. ( 2):', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': 'EQUATION', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': 'where:', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': '\u03c3 t represents the EWMA volatility, defined as the standard deviation of the exponentially weighted log returns.', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': '-\u03b1 = 2', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': '\u03c4+1 is the smoothing factor, controlling the decay of weights.', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': '\u03c4 is the window length (set to 30 days in this study).', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': '-N is the maximum number of terms in the dataset. The EWMA volatility emphasizes recent price dynamics by assigning exponentially decreasing weights to older log returns, ensuring that more recent movements have a greater influence on the volatility estimation.', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': '\u2022 Barrier Calculation and Label Assignment: Upper and lower barriers are calculated using the historical volatility (\u03c3 t ) derived in Eq. ( 2) as a coefficient to the barrier factors, enabling forward-looking estimations without introducing look-ahead bias. The upper and lower barriers for a given time t are calculated as shown in Eq. ( 3) and Eq. ( 4), respectively:', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': 'EQUATION', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': 'where:', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': '-P t is the closing price at time t, -\u03c3 t is the historical volatility at time t, calculated using Eq. ( 2), -F u and F l are the upper and lower barrier factors, optimized based on market dynamics. A vertical barrier (V t ) is defined as the maximum allowable time horizon for labeling, ranging from 8 to 15 days. Label assignment is determined based on the following rules:', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': '-A Bullish label is assigned if P t \u2265 U t before reaching V t . -A Bearish label is assigned if P t \u2264 L t before reaching V t . -A Neutral label is assigned if neither U t nor L t is crossed', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': 'by the time V t is reached. Once a barrier (U t or L t ) is touched, the labeling window closes, and a new window is initiated with updated U t , L t , and V t values recalculated from the historical volatility at that time. This approach ensures the method remains forward-looking and avoids look-ahead bias, as each decision relies solely on information available up to the current point in time.', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': '\u2022 Optimization and Trading Strategy: The method optimizes three parameters-upper and lower barrier factors (F u , F l ) and the vertical barrier length (V t )-over six-month intervals using the Sharpe Ratio (SR) calculated as shown by Eq. ( 5): where R p is the portfolio return, R f is the risk-free rate (set to 4% annually), and \u03c3 p is the standard deviation of portfolio returns. A 365-day year is assumed. The optimization ensures that barriers (U t , L t , and V t ) are only valid if touched by the high or low price within V t . A minimum trend duration of two days is enforced to enhance labeling robustness. For a bullish window, a long position is opened at the start of the window and held until the window ends. For a bearish window, a short position is opened at the start of the window and closed at the end of the window. No trades are executed during neutral windows.', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': 'SR = E[R p -R f ] \u03c3 p (5)', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': 'This labeling process aligns with the typical time frame of social media driven market impact, providing an effective decisionmaking window for trading agents. It addresses challenges in early-window predictions while becoming more reliable as the window progresses. Figure 2 illustrates this method, with green and red lines indicating upper and lower barriers, and assigned labels as markers. By directly linking textual inputs and price changes to market movements, this approach overcomes traditional FSA limitations and enhances predictive and actionable utility.', 'section': 'Market-derived Labeling', 'sec_num': '4.2'}, {'text': \"We introduce three language model classifiers tailored to predict short-term market trends using our novel market-derived labeling approach. The primary objective is to assess whether language models can directly predict market movements from tweets and how they compare against conventional sentimentbased classifiers. Initially, we evaluate the capability of language models to directly predict market trends by examining whether This technique has proven effective in financial forecasting and sentiment analysis [5] . We leverage prompt tuning to enhance the predictive power of a pre-trained language model without altering its architecture [31] . The features added to each tweet's prompt include the ROC, RSI, and the trend of the previous time window, selected due to having the highest correlations with market trends among the available features (0.26 for previous window trend, 0.14 for ROC, and 0.13 for RSI). These indicators are transformed into descriptive terms, forming prompts like:\", 'section': 'Short-Term Trend Classification of Tweets', 'sec_num': '4.3'}, {'text': 'Previous Label: bullish, ROC: neutral, RSI: bearish, Tweet: Bitcoin is on the rise!', 'section': 'Short-Term Trend Classification of Tweets', 'sec_num': '4.3'}, {'text': \"The Context-Aware (CA) model, is an evolution of the CUA model, fine-tuned using these enriched prompts. Recognizing the importance of temporal context in market analysis, we further refine our model by incorporating the date of each tweet into the prompts. Language models typically do not possess inherent temporal awareness due to their non-sequential processing [28, 32] . By adding this temporal element, we aim to enhance the model's sensitivity to time-specific market conditions, leading to the development of the Temporal Context-Aware (TCA) model. An example of this prompt includes:\", 'section': 'Short-Term Trend Classification of Tweets', 'sec_num': '4.3'}, {'text': 'Date: 2020, January, 01, Previous Label: bullish, ROC: neutral, RSI: bearish, Tweet: Bitcoin is on the rise! Through these methodologies, we expand the feature space available to the language model, enabling it to discern complex interrelations between textual content, market indicators, and temporal dynamics, thereby improving predictive accuracy in FSA.', 'section': 'Short-Term Trend Classification of Tweets', 'sec_num': '4.3'}, {'text': 'The classification of tweets is only the first step in the FSA pipeline. Once tweets are classified, the next task is to aggregate these results over a given timeframe to generate a definitive trading signal. These signals can then be used to take trading positions directly or as input to more complex models in combination with price-based or other signals. However, in this study, we focus on using our model as a single, end-to-end short-term trend classifier, excluding fusion with price-based predictions since our approach already integrates market-related information into the prompts. For comparative purposes, we also evaluate a price-based LSTM model similar to the one proposed by PreBit [20] , and a pre-trained time-series masking autoencoder model inspired by [33, 34] a simple fusion model that combines sentiment and price signals into one. This allows us to compare their performance with the signal generation methods derived from tweet classifications. We propose two methods for signal generation based on tweet aggregation, the majority and mean methods. These methods help generate both the direction and size of each trading position, which are later used in our backtesting experiments. The majority vote method aggregates daily tweet predictions by determining the majority sentiment class for a given day. Let N denote the total number of tweets in a day, and let N bullish , N bearish , and N neutral represent the number of tweets classified as Bullish, Bearish, and neutral, respectively. The majority signal, denoted as S majority , is defined by Eq. ( 6):', 'section': 'Signal Generation and Backtesting Strategies', 'sec_num': '4.4'}, {'text': 'EQUATION', 'section': 'Signal Generation and Backtesting Strategies', 'sec_num': '4.4'}, {'text': 'The confidence of the majority class, C majority , is a measure of how dominant this class is among all classified tweets. It is calculated as given by Eq. ( 7) the proportion of tweets belonging to the majority class:', 'section': 'Signal Generation and Backtesting Strategies', 'sec_num': '4.4'}, {'text': 'EQUATION', 'section': 'Signal Generation and Backtesting Strategies', 'sec_num': '4.4'}, {'text': 'Confidence reflects the certainty of the aggregated sentiment signal. Higher confidence indicates that a significant portion of the tweets for the day align with the majority sentiment, whereas lower confidence suggests greater variability in sentiment. This confidence score is used as a factor in determining the size of the trading position. To ensure comparability, the confidence values are normalized on a scale of 0 to 1 using min-max normalization. This step adjusts the confidence scores relative to the range of values observed across the dataset. The mean method aggregates tweet predictions by averaging the encoded class values over a day. In this method, class encodings are defined as follows: Bearish = 0, Neutral = 1, and Bullish = 2. Let E i denote the encoded class of the i th tweet. The mean sentiment for a day, denoted as D mean , is calculated by Eq. ( 8):', 'section': 'Signal Generation and Backtesting Strategies', 'sec_num': '4.4'}, {'text': 'EQUATION', 'section': 'Signal Generation and Backtesting Strategies', 'sec_num': '4.4'}, {'text': 'The thresholds T bearish and T bullish are used to determine the overall daily signal in comparison to D mean as given by Eq. 9, and the thresholds in this method are optimized every 6 months to match the market preferences:', 'section': 'Signal Generation and Backtesting Strategies', 'sec_num': '4.4'}, {'text': 'EQUATION', 'section': 'Signal Generation and Backtesting Strategies', 'sec_num': '4.4'}, {'text': \"The confidence score for this method is calculated as the absolute difference between the daily mean D m ean and the corresponding threshold, reflecting how strongly the mean aligns with a specific sentiment. As with the majority vote method, the confidence scores are normalized to facilitate consistent position sizing across different days. Both methods ultimately provide a daily sentiment signal S and its corresponding confidence C, which are then used in backtesting to evaluate the performance of the trading strategy. By linking confidence to position size, the approach ensures that trading decisions reflect the model's certainty in its predictions.\", 'section': 'Signal Generation and Backtesting Strategies', 'sec_num': '4.4'}, {'text': \"We propose three primary strategies for evaluating the effectiveness of our sentiment-driven trading signals. The first strategy, the TBL strategy, uses volatility-based dynamic barriers (as defined by Eq. ( 3), and Eq. ( 4)) to define take-profit and stop-loss levels, allowing for adaptive trading decisions based on market conditions. Optimized parameters for barrier thresholds and time limits are tuned for each evaluation period to maximize strategy performance. The second set of strategies referred to as In-Out strategies, employ aggregated daily signals to determine entry and exit points. The In-Out-Long strategy opens long positions when a Bullish signal is detected and closes them upon receiving a Bearish signal, while the In-Out-Short strategy operates inversely, opening short positions with Bearish signals and closing them with Bullish ones. These strategies incorporate normalized confidence scores derived from the sentiment classification process to scale position sizes, ensuring that trades reflect the model's certainty in its predictions. Additionally, we use a baseline buy-and-hold strategy for comparison, which provides a simple benchmark to contextualize the performance of the proposed methods. The backtesting process is conducted using the vectorbt library 5 , offering a comprehensive and efficient framework for evaluating trading strategies across varying market conditions. Recognizing the data dependency of backtesting results, we evaluate the proposed strategies over multiple market regimes, spanning different periods of bullish, bearish, and neutral trends. This ensures a robust assessment of strategy performance and generalizability. Key performance metrics include profitability measures, such as cumulative return and daily return; risk measures, including maximum drawdown and volatility; and profit-to-risk ratios like the Sharpe and Sortino ratios. By analyzing these metrics, we identify the strengths and weaknesses of each strategy relative to the buy-and-hold baseline, providing insights into the applicability of sentiment-driven trading signals in diverse market environments. 5 github.com/polakowo/vectorbt 5 Results\", 'section': 'Signal Generation and Backtesting Strategies', 'sec_num': '4.4'}, {'text': 'In this section, we first compare the performance of traditional sentiment models against our proposed approach in classifying tweets on the proposed short-term trend labels. Then, we generate daily trading signals based on the aggregation of daily predictions of the proposed model and compare them to the price-based classification models and conventional fusion models utilizing price and sentiment features. Finally, the proposed trading strategies are evaluated through backtesting across various market conditions.', 'section': 'Signal Generation and Backtesting Strategies', 'sec_num': '4.4'}, {'text': 'This experiment investigates the ability of language models to predict short-term market trends as obtained by the proposed labeling method. The primary objective is to evaluate whether fine-tuning sentiment models on market-derived labels and incorporating contextual information can enhance the performance of traditional sentiment models in forecasting short-term market movements. The models are fine-tuned on a dataset from 2020 (E a ), consisting of approximately 60,000 tweets evenly distributed across the three trend classes, and evaluated on both E a and an event-sampled dataset (E b ). The E b dataset includes around 40,000 tweets from 2015 to 2023 (excluding 2020) and focuses on tweets related to significant Bitcoin events. This dataset represents high-volatility, news-rich scenarios, providing an opportunity to test the robustness of the models under diverse market conditions. To ensure a rigorous evaluation, grouped 5-fold cross-validation is employed, preventing information leakage by grouping tweets based on their temporal proximity. The training is conducted over 2 epochs per fold, as experiments demonstrate that performance plateaus beyond this point, indicating that the models effectively converge within this limit. Baseline models, FinBERT [14] and CryptoBERT [15] , respectively called Base-1, and Base-2 in Table 1 , exhibit poor performance in predicting short-term market trends, achieving F1-scores of 17.2% and 27.7% on E a , respectively. These results highlight the limitations of sentiment-based models in capturing short-term market movements. This underperformance can be attributed to the fact that these models are trained to interpret sentiment labels based on human annotations, which reflect subjective sentiment rather than actual market behavior. Consequently, the models cannot connect textual features with market-derived outcomes, as they are not explicitly trained on labels derived from market movements. In contrast, the CUA model, fine- tuned with the proposed market-derived labels, demonstrates significant improvement, achieving an F1-score of 43.0% on E a highlighting the importance of using market-derived labeling to bridge the gap between textual features and market dynamics.', 'section': 'Tweet Classification Evaluation', 'sec_num': '5.1'}, {'text': 'Further performance gains are achieved through prompt tuning with market context. The CA model, which incorporates features such as the previous trend, ROC, and RSI as textual input, achieves F1-scores of 89.5% on E a and 80.3% on E b .', 'section': 'Tweet Classification Evaluation', 'sec_num': '5.1'}, {'text': 'Compared to the CUA model, the CA model demonstrates an impressive improvement across all evaluation metrics, highlighting the pivotal role of descriptive market context in bridging the gap between textual and market data. These results show that when integrated as textual features, market context empowers the language model to establish patterns between market context, tweet content, and ensuing market trends far more effectively. This improvement is not merely incremental but transformative, as it validates the initial hypothesis that language models when equipped with descriptive market context, can function as end-to-end predictors for short-term trends without the need for fusion with price-based models.', 'section': 'Tweet Classification Evaluation', 'sec_num': '5.1'}, {'text': 'As shown in Table 1 , the TCA model, which incorporates temporal context alongside market features, demonstrates notable performance on the 2020 dataset (E a ), with an F1-score of 86.5% (\u00b16.9%), and on the event-sampled dataset (E b ), achieving an F1-score of 71.0% (\u00b11.4%). While the model performs well on E a , its performance on E b is noticeably lower, suggesting that the inclusion of temporal context may increase the risk of overfitting to the training data. This reduced performance on the unseen event-sampled data indicates that the model may not generalize as effectively as the CA model, which performs better on E b with an F1-score of 80.3% (\u00b10.2%). The CA model also performs well on E a with an F1-score of 89.5% (\u00b113.2%) and on E b with a score of 80.3% (\u00b10.2%), but similarly exhibits considerable variability. This variability in performance across folds is more pronounced in the CA and TCA models compared to the base models, where performance remains more stable. These variations highlight the sensitivity of the models to specific data splits. To mitigate these fluctuations, we adopt a majority voting strategy for the subsequent experiments, where predictions are aggregated across all five folds. This ensemble learning approach reduces the influence of fold-specific anomalies, enhances stability, and provides a more reliable evaluation of model performance. Additionally, paired t-tests comparing the CA and CUA models against Base-1 (CryptoBERT) model across the five folds confirm statistically significant differences in performance (p < 0.001 for both comparisons), further vali-dating the robustness and superior performance of the proposed models.', 'section': 'Tweet Classification Evaluation', 'sec_num': '5.1'}, {'text': 'To further investigate the contribution of individual context components (previous label, ROC, and RSI), we systematically exclude each feature from the prompts and measure the resulting accuracy. Excluding the previous label causes the model\\'s accuracy to drop significantly to 43.6%, highlighting its pivotal role in aligning tweet content with past market trends. On the other hand, removing RSI and ROC results in accuracies of 79.0% and 78.8%, respectively, indicating that these features, while less critical than the previous label, still contribute substantially to model performance. To confirm that the model is not overly reliant on prompts and effectively interprets tweet content, we conducted extensive SHAP value analysis [35] on a diverse range of tweets. Our tests revealed a wide variety of contributions from different parts of the tweets, demonstrating the model\\'s ability to adapt to varying linguistic patterns and contexts. Figure 3 illustrates one such sample from our analysis, showcasing how specific words influence the model\\'s prediction. In this example, despite a Bullish previous label, the model predicts a Bearish outcome primarily due to the strong contribution of the word \"downward\" in the tweet. The SHAP values indicate that \"downward\" exerts a dominant influence on the prediction, strongly shifting the model\\'s decision toward bearishness. This analysis confirms that the model effectively processes tweet content, ensuring predictions are based on meaningful contextual analysis rather than superficial reliance on the prompt information. The results validate the robustness of the model in integrating contextual information and analyzing tweet content for accurate market predictions.', 'section': 'Tweet Classification Evaluation', 'sec_num': '5.1'}, {'text': 'The signal generation process translates tweet classifications into actionable trading signals, providing a clear recommendation on the short-term market movement. Unlike the classification task, which focuses on individual tweets, this step aggregates predictions from all tweets in a given day to determine the dominant signal. Traditional methods for signal generation often rely on multi-stage architectures, where sentiment predictions are combined with market-derived features in a separate fusion model to return a final prediction of how the market will move. While effective in some cases, such architectures fail to capture nuanced interactions between textual and market features, leading to potential information loss and requiring more data and a more complex architecture. In contrast, our CA language model integrates market features (e.g., previous trend, ROC, RSI) directly into tweet prompts as textual descriptions, allowing for an end-to-end prediction process that inherently models these interactions. This experiment evaluates whether this innovative approach generates reliable signals and how it compares to traditional price-based and fusion-based prediction models.', 'section': 'Signal Generation', 'sec_num': '5.2'}, {'text': \"For comparative analysis, we utilize two baseline price-based models, Base-3 (LSTM model inspired by [20] ) and Base-4 (Autoencoder model inspired by [33, 34] ), along with a fusion model that combines predictions from the base CryptoBERT model, price-based models, and our proposed majority CA model to generate short-term market signals. For each tweet, our proposed model generates predictions using an ensemble majority voting method across a 5-fold cross-validation setup, ensuring data diversity among the models. These individual predictions are then aggregated daily using either the majority or mean aggregation methods. The fusion model takes the daily predictions based on the majority aggregation method as input, alongside traditional sentiment and price-based predictions, to ensure it has access to all available features. The final output from the model is a predicted class (bullish, bearish, or neutral) for each day. For sentiment features, the fusion model receives the ratio of bullish to bearish tweets, represented as a continuous value between 0 and 1, which has proven to be more effective than using the final prediction class. Similarly, price-based models provide a final predicted class utilizing features such as OHLCV, Ethereum price, gold price, RSI, ROC, MACD, and moving averages (SMA7, SMA21, EMA12, EMA26). These three input features (the CA model's aggregated predictions, the sentiment ratio, and the price-based predictions) are then combined in the fusion model to generate the final market prediction, which is used to assess short-term market movements.\", 'section': 'Signal Generation', 'sec_num': '5.2'}, {'text': \"In Table 2 , Metrics accuracy, precision, recall, and F1-score are presented in both One-Vs-Rest (OVR) and One-Vs-One (OVO) modes for our benchmarks and proposed language-based models. The OVR mode assessed the models' ability to distinguish between Bullish, Bearish, and Neutral predictions. In contrast, the OVO mode focused on pairwise comparisons between Bullish and Bearish predictions (presented with OVO + for bullish against bearish, and OVO -for bearish against bullish The majority and mean aggregation methods and the fusion method demonstrate their precision, recall, and F1-scores.\", 'section': 'Signal Generation', 'sec_num': '5.2'}, {'text': \"metrics), which have a greater trading impact. This approach ensured a comprehensive evaluation of the models' capabilities, with the OVO results highlighting sensitivity to critical decisionmaking scenarios. The results, presented in Table 2 , indicate that the CA model achieved comparable or superior performance to the fusion and price-based models. For example, the majority vote aggregation of the CA model attained an F1-score of 54.21%, outperforming Base-3 model (49.20%) and Base-4 model (50.01%). Notably, the majority method demonstrated balanced performance across Bullish and Bearish predictions, achieving F1-scores of 82.62% for Bearish, and 86.12% for Bullish predictions and consistent precision and recall across these scenarios. In contrast, while the fusion model performed slightly worse in Bullish scenarios (F1: 85.28%), it struggled significantly with Bearish predictions (F1: 59.34%).\", 'section': 'Signal Generation', 'sec_num': '5.2'}, {'text': 'These findings underscore the adaptability of the proposed method in dynamic market environments and its ability to serve as a reliable end-to-end prediction model. By aggregating tweet classifications with market context descriptors, the proposed method bypasses the need for separate fusion architectures and demonstrates the potential of language models to capture complex interactions between textual and market data. This opens the door to leveraging language models for signal generation in financial forecasting, reducing reliance on multi-stage architectures and offering a streamlined, effective alternative. An important observation from the confusion matrix of the proposed majority method is the nature of misclassifications. Only 5.1% of Bullish labels are misclassified as Bearish, and 12.5% of Bearish labels are misclassified as Bullish. This demonstrates that the model avoids direct, high-impact errors between Bullish and Bearish labels-errors that can lead to substantial trading risks. Instead, most misclassifications occur when Bullish or Bearish The majority method maintains high precision and recall while minimizing risky misclassifications highlighting its robustness and adaptability.', 'section': 'Signal Generation', 'sec_num': '5.2'}, {'text': \"After generating signals, we aim to evaluate the performance of our proposed methods in terms of their ability to profit from the market through a few simple trading strategies. A significant challenge in evaluating trading strategies is their dependence on the prevailing market regime, which can influence performance. To ensure a comprehensive assessment, we test our strategies across three distinct market regimes, as shown in Figure 5 . These regimes include a Bullish regime from 2017 to 2018, characterized by rapid market growth of approximately 1200%, a Bearish regime from 2018 to 2019, marked by a sharp decline of 250%, and a Neutral regime from mid-2019 to mid-2020, which demonstrated moderate stability with around a 40% increase. The proposed signal generation methods are applied across three trading strategies. To contextualize these methods, we compare their performance against the buy-and-hold and sell-and-hold benchmarks, which offer a baseline representing passive strategies that follow market trends. By testing these strategies across diverse market conditions, we aim to understand the adaptability and robustness of the proposed methods in generating actionable trading signals. The results presented in Table 3 highlight several notable patterns. During the Bullish period, the benchmark buy-and-hold strategy delivers an impressive Sharpe ratio of 3.14 and a daily return of 3.38%, reflecting Bitcoin's remarkable upward momentum. Despite the benchmark's strong performance, four out of six proposed strategies exceeded this Sharpe ratio, with all strategies outperforming the benchmarks across the other two market regimes, demonstrating robust adaptability. Notably, the Majority TBL strategy achieved the highest Sharpe ratio in both the Bullish and Bearish mar- kets, with a Sharpe ratio of 5.07 in the former and 4.70 in the latter. But, As shown in Figure 6 in the stagnant market regime, the Mean TBL method outperformed the others with a Sharpe ratio of 3.73, creating a significant gap over the benchmark.\", 'section': 'Trading Strategy', 'sec_num': '5.3'}, {'text': 'Profiting in stagnant markets is notoriously challenging, but the cumulative return plot in Figure 6 demonstrates a consistent and steady upward trend, showing the robust profitability of our proposed method. As for the In-Out strategies, they exhibit a strong dependence on market regimes, as expected given their one-sided trading approach. For instance, the In-Out-Long strategy excelled during the bullish regime, delivering high daily returns, but underperformed in bearish conditions. These strategies had longer trade durations than TBL strategies, increasing risk exposure, and executed fewer trades, providing respectable Sharpe ratios-except when operating against market trends. Among the proposed methods, the TBL strategies emerged as the top performers, with shorter trade durations (typically 2-5 days), smaller maximum drawdowns, and reduced risk exposure. They execute more frequent trades than the In-Out strategies and consistently deliver higher Sharpe and Sortino ratios, demonstrating stable performance across different regimes, including profitability in the stagnant neutral markets. Regarding aggregation methods, while the majority and mean approaches deliver comparable overall results, the majority method excelled in trending markets (bullish or bearish), whereas the mean method shows better performance in neutral markets. Finally, testing the Majority TBL and Mean TBL strategies in January 2021-the first month after the training period and a bullish phase; yielded Sharpe ratios of 4.43 and 3.49, respectively, aligning well with their expected performance in such conditions.', 'section': 'Trading Strategy', 'sec_num': '5.3'}, {'text': \"These results underscore the practicality and robustness of our language model-driven predictions in backtesting environments. Notably, the fine-tuning process, designed specifically with TBL market-derived labels, naturally aligns the model's strengths with the TBL strategy. This demonstrates that the choice of labeling method can serve as a strategic advantage, enabling the use of strategies that directly match the labeling process to achieve superior results. The TBL strategy stands out as a reliable approach for leveraging model predictions, offering a balanced risk profile and adaptability to a diverse set of market conditions.\", 'section': 'Trading Strategy', 'sec_num': '5.3'}, {'text': \"This section examines the limitations and implications of the proposed approach. We also highlight the benefits of marketderived labeling, context-rich prompts, and language models' potential to enhance financial market predictions and decisionmaking based on the findings of this study.\", 'section': 'Discussion', 'sec_num': '6'}, {'text': 'Our approach assumes a direct short-term market impact for each tweet, which may oversimplify the intricate and often indirect relationship between social media activity and market movements. By treating tweets as market movement descriptors rather than delving into their underlying intent, this method risks overlooking the nuanced factors that drive market behavior. Furthermore, the association between specific textual patterns and market trends may be transient, requiring frequent model updates to maintain performance. The reliance on prompt engineering introduces the risk of overfitting, as the model might become overly attuned to training data patterns, thereby limiting its ability to generalize to new data. While SHAP analysis indicates effective use of tweet content, reducing numerical features to categorical prompts may lose some of the fine-grained details critical for identifying complex patterns, leaving traditional numerical models potentially superior in certain contexts. Despite these challenges, our experiments demonstrate that directly labeling tweets with market outcomes significantly enhances predictive power compared to traditional sentiment-based approaches, offering a more pragmatic framework that prioritizes measurable market impacts in financial social media analysis.', 'section': 'Limitations', 'sec_num': '6.1'}, {'text': 'This study provides insights into the application of sentiment analysis for financial market forecasting, underscoring the need for more context-aware and market-focused approaches. The results highlight the effectiveness of tailoring labeling methods to align with the specific objectives of FSA, emphasizing that predicting market movements rather than merely assessing sentiment, requires a market-derived labeling approach. This finding aligns with previous research [13] and reinforces the paradigm shift toward directly linking textual content to market behavior [2] , bypassing the limitations of intermediate sentiment evaluations. Integrating market context through prompt tuning emerges as another pivotal factor in enhancing predictive performance. By embedding key market metrics such as price trends and volatility indicators directly into text prompts, the model captures a richer understanding of the conditions shaping market movements. This approach not only improves prediction accuracy but also demonstrates the potential for augmenting language models with external data sources to achieve even greater efficacy in financial forecasting tasks. Furthermore, the ability of the proposed model to generate actionable trading signals demonstrates its practical utility in real-world financial decision-making. Traders and investors could use these models to inform strategies based on real-time social media analysis, paving the way for more efficient trading practices and enhanced risk management.', 'section': 'Implications', 'sec_num': '6.2'}, {'text': '7 Conclusions and Future Work', 'section': 'Implications', 'sec_num': '6.2'}, {'text': \"In this study, we demonstrate that a direct, market-based approach to classifying tweets outperforms traditional sentimentbased models in predicting Bitcoin's short-term market movement. Traditional sentiment analysis often fails to bridge the gap between textual sentiment and actual market behavior, as evidenced by the poor performance of the CryptoBERT [15] and FinBERT [14] models, which achieved F1-scores of only 25.2% and 16.7%, respectively, on a dataset of historical Bitcoin events. In contrast, by using market-derived labeling-defining clear profit/loss and temporal boundaries-we establish a direct link between text and market outcomes. Fine-tuning language models with these market-derived labels improved the F1-score by 15.3%, and further integrating market context through prompttuning boosted performance by a staggering 47%, reaching an F1-score of 80.3%. We also show that our model, which aggregates the predictions of the context-aware language model into actionable signals, can act as an effective standalone short-term market signal generator. Compared to the traditional fusion model approach, our method consistently outperforms benchmarks, achieving Sharpe ratios of up to 5.07 in trending markets and 3.73 in neutral conditions. These results highlight the untapped potential of language-based models for financial fore-casting. These findings demonstrate the transformative potential of language models in financial forecasting. These models can produce actionable insights, generate effective trading strategies, and manage risk in highly volatile markets by moving beyond sentiment classification to integrate context and market signals.\", 'section': 'Implications', 'sec_num': '6.2'}, {'text': \"We recommend an in-depth exploration of language-based models for financial decision-making, moving beyond their integration as mere features within traditional numerical forecasting models. Future research could focus on three key areas: First, leveraging LLMs to power chatbots for investment advisory services, assisting financially inexperienced users in navigating investment strategies, best practices, and financial knowledge. Second, advancements in Retrieval-Augmented Generation (RAG) can enable an interactive, query-based interface for financial reports, offering faster, more intuitive access to critical asset information and potentially democratizing detailed financial analysis. Third, improvements in prompt tuning, along with novel feature engineering-such as incorporating tweet author reliability, social media engagement, and technical indicators-could substantially elevate the performance of these models. Additionally, personalizing financial advice by tailoring recommendations to individual profiles, risk tolerance, and goals could transform how financial planning is approached. While the potential of language models in finance is significant, their value must be substantiated through robust evaluation frameworks. These frameworks must simulate real-world conditions, incorporating transaction costs, varying market regimes, and other practical considerations, to fully assess the models' real-world applicability and effectiveness in financial decision-making.\", 'section': 'Implications', 'sec_num': '6.2'}, {'text': 'github.com/hamidm21/Revisit_FSA', 'section': '', 'sec_num': None}, {'text': 'kaggle.com/datasets/hamidmoradi21/bitcoin-historical-events', 'section': '', 'sec_num': None}, {'text': 'kaggle.com/datasets/kaushiksuresh147/bitcoin-tweets', 'section': '', 'sec_num': None}, {'text': 'Open, High, Low, Close, and Volume', 'section': '', 'sec_num': None}], 'back_matter': [], 'ref_entries': {'FIGREF0': {'fig_num': '1', 'type_str': 'figure', 'text': 'Figure 1: Overall scheme of the proposed approach', 'num': None, 'uris': None}, 'FIGREF1': {'fig_num': '2', 'type_str': 'figure', 'text': 'Figure 2: Visualization of the proposed labeling method. The green and red lines represent the upper and lower barriers respectively, while the markers denote assigned labels for the given trend windows (denoted with vertical dotted lines).', 'num': None, 'uris': None}, 'FIGREF2': {'fig_num': '3', 'type_str': 'figure', 'text': \"Figure 3: SHAP values plot for a sample tweet. The plot shows the contribution of each word to the model's prediction, highlighting the significant impact of the RSI feature on the Bearish prediction.\", 'num': None, 'uris': None}, 'FIGREF3': {'fig_num': '4', 'type_str': 'figure', 'text': 'Figure 4: Precision-recall scatter plot comparing performance across different methods for Bullish and Bearish predictions. The majority and mean aggregation methods and the fusion method demonstrate their precision, recall, and F1-scores.', 'num': None, 'uris': None}, 'FIGREF4': {'fig_num': '5', 'type_str': 'figure', 'text': 'Figure 5: Yearly periods selected to represent three distinct market regimes: Bullish, Bearish, and Neutral.', 'num': None, 'uris': None}, 'FIGREF5': {'fig_num': '6', 'type_str': 'figure', 'text': 'Figure6: Backtest visualization of the Mean TBL strategy during the neutral period, illustrating entry and exit points, as well as the profit and loss for each trade. The chart highlights the underperformance of the buy-and-hold strategy in stagnant market conditions, while demonstrating the effectiveness of our proposed method in navigating such environments.', 'num': None, 'uris': None}, 'TABREF1': {'type_str': 'table', 'html': None, 'text': '3 , extends from February 2021 to March 2023. For each day, tweets were combined and preprocessed through steps including text normalization (lowercasing, removal of URLs, user IDs, and punctuation), extraction of emojis and hashtags as sentiment indicators, and removal of promotional and advertisement content to reduce noise. Lemmatization is also applied to unify terms and improve generalization. \u2022 Technical Indicators: The Relative Strength Index (RSI) and Rate of Change (ROC) are calculated from OHLCV 4 data and are used as technical indicators for prompt tuning. Thresholds of 30 and 70 are set for RSI to signal oversold and overbought conditions. ROC thresholds are dynamically adjusted based on the standard deviation of eight-day returns to balance the dataset and align with RSI levels, providing more robust trend insights and identifying potential reversals. The same OHLCV data, spanning from January 2015 to March 2023, was used to produce the daily labels.', 'num': None, 'content': '<table/>'}, 'TABREF2': {'type_str': 'table', 'html': None, 'text': \"tweets contain learnable features indicative of subsequent market movements. We start by benchmarking traditional sentiment models, specifically FinBERT and CryptoBERT, against our dataset labeled with market-derived trends. This initial step sets the baseline for understanding if sentiment alone can predict market shifts. Following this, we fine-tune the CryptoBERT model on these market-derived labels, creating what we call the Context-UnAware (CUA) model. This model is designed to detect patterns in tweet content that correlate with market trends, preserving the model's existing knowledge of cryptocurrency jargon and tweet structures. The fine-tuning process involves freezing the first 11 layers of CryptoBERT to maintain this domain-specific knowledge while only adjusting the classification head. The training was conducted over 2 epochs with a 5-fold cross-validation strategy to avoid overfitting, using the AdamW optimizer with a learning rate of 10 -5 and a batch size of 12. A learning rate scheduler with a linear warm-up phase (10% of steps) was employed to facilitate gradual learning. Building on the insights from previous research on the synergy between market context and textual content[3], we then introduce market context into our model's input through prompt engineering.\", 'num': None, 'content': '<table/>'}, 'TABREF3': {'type_str': 'table', 'html': None, 'text': 'Evaluation of base models Base-1, and Base-2, (respectively FinBERT and CryptoBERT) and proposed methods on the 2020 5-fold cross-validation dataset (E a ) and event-sampled Bitcoin data (E b ). The results for E a are the averages of metrics across 5 folds, with standard deviations representing fold-to-fold variation. For the base models, E a results reflect variations across the different data folds.', 'num': None, 'content': '<table><tr><td>Model</td><td>E a</td><td colspan=\"2\">Accuracy</td><td>E b</td><td>E a</td><td>Precision</td><td>E b</td><td>E a</td><td>Recall</td><td>E b</td><td>E a</td><td>F1-Score</td><td>E b</td><td>Cross-entropy Loss E a E b</td></tr><tr><td colspan=\"3\">Base-1 [14] 33.3 \u00b1 0.6</td><td colspan=\"2\">32.8</td><td colspan=\"2\">33.8 \u00b1 2.3</td><td>26.4</td><td colspan=\"2\">33.3 \u00b1 0.1</td><td>33.2</td><td colspan=\"2\">17.2 \u00b1 0.3</td><td>16.7</td><td>2.06 \u00b1 5.1</td><td>2.41</td></tr><tr><td colspan=\"3\">Base-2 [15] 33.3 \u00b1 1.3</td><td colspan=\"2\">28.2</td><td colspan=\"2\">32.2 \u00b1 1.9</td><td>30.8</td><td colspan=\"2\">33.3 \u00b1 0.9</td><td>32.1</td><td colspan=\"2\">27.7 \u00b1 0.8</td><td>25.2</td><td>2.28 \u00b1 9.1</td><td>2.37</td></tr><tr><td>CUA</td><td colspan=\"14\">44.1 \u00b1 1.3 35.3 \u00b1 0.5 44.7 \u00b1 1.9 34.4 \u00b1 0.7 44.1 \u00b1 1.3 35.8 \u00b1 0.4 43.0 \u00b1 2.2 33.3 \u00b1 0.8 1.04 \u00b1 0.12 1.14 \u00b1 0.15</td></tr><tr><td>CA</td><td colspan=\"14\">89.6 \u00b1 9.6 81.0 \u00b1 0.2 90.3 \u00b1 12.1 80.4 \u00b1 0.3 89.3 \u00b1 14.4 80.2 \u00b1 0.2 89.5 \u00b1 13.2 80.3 \u00b1 0.2 0.40 \u00b1 0.05 1.24 \u00b1 0.11</td></tr><tr><td>TCA</td><td colspan=\"14\">86.5 \u00b1 8.6 71.2 \u00b1 1.3 86.6 \u00b1 5.9 71.2 \u00b1 1.0 86.5 \u00b1 7.9 71.8 \u00b1 1.4 86.5 \u00b1 6.9 71.0 \u00b1 1.4 0.72 \u00b1 0.06 0.76 \u00b1 0.07</td></tr></table>'}, 'TABREF4': {'type_str': 'table', 'html': None, 'text': 'Evaluation of benchmark price-based models (Base-3, and Base-4), and fusion models, alongside majority and mean signal generation methods based on the proposed languagebased prediction model (CA).', 'num': None, 'content': '<table><tr><td/><td>Model</td><td colspan=\"3\">Accuracy Precision Recall</td><td>F1</td></tr><tr><td/><td>Base-3 [20]</td><td>52.87</td><td>46.76</td><td colspan=\"2\">52.87 49.20</td></tr><tr><td>OVR</td><td>Base-4 [33, 34] Fusion</td><td>54.25 59.01</td><td>51.31 58.35</td><td colspan=\"2\">54.63 55.14 55.94 50.01</td></tr><tr><td/><td>Majority (CA)</td><td>53.52</td><td>61.15</td><td>57.46</td><td>54.21</td></tr><tr><td/><td>Mean (CA)</td><td>57.91</td><td>56.50</td><td colspan=\"2\">57.13 56.10</td></tr><tr><td>+</td><td>Fusion</td><td>78.38</td><td>80.72</td><td colspan=\"2\">90.38 85.28</td></tr><tr><td>OVO</td><td>Majority (CA) Mean (CA)</td><td>84.57 76.95</td><td>84.30 83.60</td><td colspan=\"2\">88.02 86.12 76.33 79.80</td></tr><tr><td>OVO -</td><td>Fusion Majority (CA) Mean (CA)</td><td>78.38 84.92 76.95</td><td>70.31 84.92 69.00</td><td colspan=\"2\">51.33 59.34 80.45 82.62 77.86 73.16</td></tr></table>'}, 'TABREF5': {'type_str': 'table', 'html': None, 'text': 'Backtest evaluation of our proposed language-model-based approach to signal generation in Bullish, Bearish, and Neutral market regimes.', 'num': None, 'content': '<table><tr><td>Regime</td><td/><td>Strategy</td><td>Daily</td><td colspan=\"3\">Sharpe Sortino Closed</td><td>Max</td><td>Profit</td><td>Win</td><td>Avg Win</td><td>Avg Losing</td></tr><tr><td/><td/><td/><td>Return %</td><td/><td/><td>Trades</td><td>DD%</td><td>Factor</td><td>Rate%</td><td>Trade Time</td><td>Trade Time</td></tr><tr><td/><td colspan=\"2\">Buy and Hold</td><td>3.38</td><td>3.12</td><td>4.52</td><td>1</td><td>35.36</td><td>-</td><td>100</td><td>367 days</td><td>-</td></tr><tr><td>Bullish</td><td>Majority</td><td>In-Out-Long In-Out-Short TBL</td><td>10.88 0.40 7.86</td><td>4.84 2.39 5.07</td><td>7.15 2.49 8.42</td><td>11 11 66</td><td colspan=\"2\">34.46 807.90 11.11 19.44 20.92 12.04</td><td>90.90 72.72 84.84</td><td>23 days 6 days 2 days</td><td>7days 8 days 3 days</td></tr><tr><td/><td>Mean</td><td>In-Out-Long In-Out-Short</td><td>6.45 0.01</td><td>4.58 0.36</td><td>5.99 0.37</td><td>17 16</td><td colspan=\"2\">21.56 290.19 61.01 1.04</td><td>93.75 68.75</td><td>12 days 7 days</td><td>6 days 18 days</td></tr><tr><td/><td/><td>TBL</td><td>4.27</td><td>3.69</td><td>6.12</td><td>79</td><td>47.04</td><td>2.51</td><td>77.21</td><td>2 days</td><td>5 days</td></tr><tr><td/><td colspan=\"2\">Buy and Hold</td><td>0.04</td><td>0.58</td><td>0.69</td><td>1</td><td>63.97</td><td>-</td><td>100</td><td>367 days</td><td>-</td></tr><tr><td>Neutral</td><td>Majority</td><td>In-Out-Long In-Out-Short TBL</td><td>0.55 0.29 2.22</td><td>3.35 1.45 3.70</td><td>6.03 1.99 5.29</td><td>6 6 33</td><td>8.09 22.48 25.05</td><td>-28.17 4.23</td><td>100 83.33 90.90</td><td>22 days 39 days 4 days</td><td>-14 days 4 days</td></tr><tr><td/><td>Mean</td><td>In-Out-Long In-Out-Short</td><td>0.62 0.29</td><td>3.60 1.41</td><td>7.32 2.18</td><td>8 8</td><td>6.62 22.94</td><td>-5.62</td><td>100 62.5</td><td>9 days 47 days</td><td>-11 days</td></tr><tr><td/><td/><td>TBL</td><td>2.81</td><td>3.73</td><td>6.17</td><td>45</td><td>25.67</td><td>3.80</td><td>77.77</td><td>5 days</td><td>5 days</td></tr><tr><td/><td colspan=\"2\">Sell and Hold</td><td>0.19</td><td>1.26</td><td>1.77</td><td>1</td><td>26.45</td><td>-</td><td>100</td><td>367 days</td><td>-</td></tr><tr><td>Bearish</td><td>Majority</td><td>In-Out-Long In-Out-Short TBL</td><td>0.43 1.23 3.72</td><td>1.75 4.21 4.70</td><td>1.92 8.48 7.62</td><td>15 15 68</td><td>49.05 9.89 33.77</td><td>10.37 69.69 7.47</td><td>73.33 93.33 72.05</td><td>11 days 10 days 2 days</td><td>16 days 7 days 3 days</td></tr><tr><td/><td>Mean</td><td>In-Out-Long In-Out-Short</td><td>0.38 1.09</td><td>1.85 3.24</td><td>1.72 4.37</td><td>17 16</td><td>36.01 32.44</td><td>4.86 17.07</td><td>70.58 93.75</td><td>8 days 13 days</td><td>8 days 14 days</td></tr><tr><td/><td/><td>TBL</td><td>2.30</td><td>3.67</td><td>5.60</td><td>84</td><td>37.68</td><td>3.08</td><td>65.47</td><td>3 days</td><td>3 days</td></tr></table>'}}}}\n\n## Task\n1. We want to reproduce the method described in the attached paper. \n2. The authors did not release any official code, so we have to plan our own implementation.\n3. Before writing any Python code, please outline a comprehensive plan that covers:\n   - Key details from the paper's **Methodology**.\n   - Important aspects of **Experiments**, including dataset requirements, experimental settings, hyperparameters, or evaluation metrics.\n4. The plan should be as **detailed and informative** as possible to help us write the final code later.\n\n## Requirements\n- You don't need to provide the actual code yet; focus on a **thorough, clear strategy**.\n- If something is unclear from the paper, mention it explicitly.\n\n## Instruction\nThe response should give us a strong roadmap, making it easier to write the code later."}, {"role": "assistant", "content": "To effectively reproduce the experiments and methodologies described in the paper \"Revisiting Financial Sentiment Analysis: A Language Model Approach,\" we need a clear and detailed plan that outlines the workflow, datasets, model configuration, training processes, and evaluation metrics. Here's a comprehensive plan that adheres strictly to the paper:\n\n---\n\n### **1. Methodology Overview**\n\n#### **A. Market-Driven Labeling Approach**\n- **Objective**: Replace subjective sentiment labels with market-derived labels that reflect real market movements.\n- **Triple Barrier Labeling (TBL)**:\n  - **Upper Barrier**: Set profit-taking price threshold for Bullish labels.\n  - **Lower Barrier**: Set stop-loss price threshold for Bearish labels.\n  - **Vertical Barrier**: Temporal constraint defining observation window (8 to 15 days).\n\n#### **B. Language Model Configuration**\n- **Models Used**: CryptoBERT and FinBERT as base models.\n- **Prompt Tuning**:\n  - Incorporate key market indicators such as the Rate of Change (ROC), Relative Strength Index (RSI), and historic market trends into textual prompts.\n  - Contextual prompts are created by embedding market and temporal context.\n\n### **2. Data Collection and Preparation**\n\n#### **A. Dataset Requirements**\n- **Bitcoin Historical Events**:\n  - 227 significant Bitcoin events from 2009 to 2024.\n  - Focus on impactful periods between 2015 and 2023.\n  - Obtain from Kaggle for reproducibility.\n- **Tweets Data**:\n  - Sources from PreBit and Kaggle from 2015 to early 2023.\n  - Used for model training and evaluation.\n\n#### **B. Preprocessing Steps**\n- Text normalization:\n  1. Lowercasing.\n  2. Removal of URLs, user IDs, and punctuation.\n  3. Extraction of emojis and hashtags.\n  4. Lemmatization for generalization.\n- **Technical Indicators**:\n  - Compute RSI and ROC from OHLCV data for prompt tuning.\n\n### **3. Experimental Setup**\n\n#### **A. Feature Engineering**\n- **Market-derived Label Assignment**:\n  - Use EWMA to estimate volatility.\n  - Calculate barriers based on historical volatility.\n  - Determine labels based on the TBL method dynamics.\n\n#### **B. Language Model Training**\n- **Fine-tuning Methodology**:\n  - Use 5-fold cross-validation.\n  - Train over 2 epochs as performance plateaus post this point.\n  - Use AdamW optimizer with learning rate \\(10^{-5}\\) and batch size of 12.\n\n### **4. Trading Signal Generation**\n\n#### **A. Signal Generation Methods**\n- **Majority and Mean Methods**:\n  - Aggregate tweet predictions by majority or average sentiment for generating day-end signals.\n  - Use confidence scores linked to position sizes.\n\n#### **B. Incorporation of Fusion Models**\n- **Price-based and Sentiment-based Models**:\n  - Compare results with LSTM and Autoencoder price-based models.\n  - Use a fusion model that incorporates sentiment predictions, and CA model aggregations.\n\n### **5. Backtesting Strategy**\n\n#### **A. Backtesting Environment**\n- Use strategies like TBL, In-Out Long/Short.\n- Conduct tests across different market regimes (Bullish, Bearish, Neutral).\n\n#### **B. Evaluation Metrics**\n- Key Performance Indicators (KPIs):\n  - Sharpe Ratio, Sortino Ratio, Max Drawdown.\n  - Profitability measures like cumulative return, daily return.\n- Tool for backtesting: Use vectorbt library for evaluation across market conditions.\n\n### **6. Evaluation and Comparison**\n\n#### **A. Classification Performance**\n- **Compare Models**:\n  - FinBERT, CryptoBERT, CA, and TCA performance on datasets \\(E_a\\) and \\(E_b\\).\n- **Metrics**:\n  - Precision, Recall, F1-Score under One-vs-Rest (OVR) and One-vs-One (OVO).\n\n#### **B. Analysis of Results**\n- Use SHAP values for interpretability.\n- Statistical validation of model improvements using t-tests.\n\n---\n\n### **Additional Notes:**\n- Ensure reproducibility via data sources and defined methodologies.\n- Address limitations such as overfitting risk with prompt engineering.\n- Consider adaptations for evolving market conditions and tweet pattern changes.\n\nThis roadmap provides a structured approach to implement and scrutinize the described methodologies, ensuring that every aspect of the paper's experiment is covered and adheres closely to the reported procedures and results."}, {"role": "user", "content": "Your goal is to create a concise, usable, and complete software system design for reproducing the paper's method. Use appropriate open-source libraries and keep the overall architecture simple.\n             \nBased on the plan for reproducing the paper\u2019s main method, please design a concise, usable, and complete software system. \nKeep the architecture simple and make effective use of open-source libraries.\n\n-----\n\n## Format Example\n[CONTENT]\n{\n    \"Implementation approach\": \"We will ... ,\n    \"File list\": [\n        \"main.py\",  \n        \"dataset_loader.py\", \n        \"model.py\",  \n        \"trainer.py\",\n        \"evaluation.py\" \n    ],\n    \"Data structures and interfaces\": \"\nclassDiagram\n    class Main {\n        +__init__()\n        +run_experiment()\n    }\n    class DatasetLoader {\n        +__init__(config: dict)\n        +load_data() -> Any\n    }\n    class Model {\n        +__init__(params: dict)\n        +forward(x: Tensor) -> Tensor\n    }\n    class Trainer {\n        +__init__(model: Model, data: Any)\n        +train() -> None\n    }\n    class Evaluation {\n        +__init__(model: Model, data: Any)\n        +evaluate() -> dict\n    }\n    Main --> DatasetLoader\n    Main --> Trainer\n    Main --> Evaluation\n    Trainer --> Model\n\",\n    \"Program call flow\": \"\nsequenceDiagram\n    participant M as Main\n    participant DL as DatasetLoader\n    participant MD as Model\n    participant TR as Trainer\n    participant EV as Evaluation\n    M->>DL: load_data()\n    DL-->>M: return dataset\n    M->>MD: initialize model()\n    M->>TR: train(model, dataset)\n    TR->>MD: forward(x)\n    MD-->>TR: predictions\n    TR-->>M: training complete\n    M->>EV: evaluate(model, dataset)\n    EV->>MD: forward(x)\n    MD-->>EV: predictions\n    EV-->>M: metrics\n\",\n    \"Anything UNCLEAR\": \"Need clarification on the exact dataset format and any specialized hyperparameters.\"\n}\n[/CONTENT]\n\n## Nodes: \"<node>: <type>  # <instruction>\"\n- Implementation approach: <class 'str'>  # Summarize the chosen solution strategy.\n- File list: typing.List[str]  # Only need relative paths. ALWAYS write a main.py or app.py here.\n- Data structures and interfaces: typing.Optional[str]  # Use mermaid classDiagram code syntax, including classes, method(__init__ etc.) and functions with type annotations, CLEARLY MARK the RELATIONSHIPS between classes, and comply with PEP8 standards. The data structures SHOULD BE VERY DETAILED and the API should be comprehensive with a complete design.\n- Program call flow: typing.Optional[str] # Use sequenceDiagram code syntax, COMPLETE and VERY DETAILED, using CLASSES AND API DEFINED ABOVE accurately, covering the CRUD AND INIT of each object, SYNTAX MUST BE CORRECT.\n- Anything UNCLEAR: <class 'str'>  # Mention ambiguities and ask for clarifications.\n\n## Constraint\nFormat: output wrapped inside [CONTENT][/CONTENT] like the format example, nothing else.\n\n## Action\nFollow the instructions for the nodes, generate the output, and ensure it follows the format example."}, {"role": "assistant", "content": "[CONTENT]\n{\n    \"Implementation approach\": \"We will develop a Python-based pipeline using open-source libraries such as TensorFlow for model creation and training, Pandas for data manipulation, and scikit-learn for evaluation metrics. The system will be designed to read raw data, preprocess it, train language models using prompt-engineering-based methods, and evaluate model performance based on market-driven labeling.\",\n    \"File list\": [\n        \"main.py\",  \n        \"dataset_loader.py\", \n        \"model.py\",  \n        \"trainer.py\",\n        \"evaluation.py\",\n        \"signal_generator.py\",\n        \"backtester.py\"\n    ],\n    \"Data structures and interfaces\": \"\nclassDiagram\n    class Main {\n        +__init__()\n        +run_experiment()\n    }\n    class DatasetLoader {\n        +__init__(config: dict)\n        +load_event_data() -> DataFrame\n        +load_tweet_data() -> DataFrame\n    }\n    class Preprocessor {\n        +__init__(settings: dict)\n        +preprocess(data: DataFrame) -> DataFrame\n    }   \n    class MarketLabeler {\n        +__init__(settings: dict)\n        +label_data(data: DataFrame) -> DataFrame\n    }\n    class Model {\n        +__init__(params: dict)\n        +forward(x: Tensor) -> Tensor\n    }\n    class Trainer {\n        +__init__(model: Model, data: DataFrame)\n        +train() -> None\n    }\n    class Evaluation {\n        +__init__(model: Model, data: DataFrame)\n        +evaluate() -> dict\n    }\n    class SignalGenerator {\n        +__init__(model: Model, settings: dict)\n        +generate_signals(data: DataFrame) -> DataFrame\n    }\n    class Backtester {\n        +__init__(signals: DataFrame, config: dict)\n        +run_backtest() -> dict\n    }\n    Main --> DatasetLoader\n    Main --> Preprocessor\n    Main --> MarketLabeler\n    Main --> Trainer\n    Main --> Evaluation\n    Main --> SignalGenerator\n    Main --> Backtester\n    Trainer --> Model\n    SignalGenerator --> Model\n\",\n    \"Program call flow\": \"\nsequenceDiagram\n    participant M as Main\n    participant DL as DatasetLoader\n    participant PR as Preprocessor\n    participant ML as MarketLabeler\n    participant MD as Model\n    participant TR as Trainer\n    participant EV as Evaluation\n    participant SG as SignalGenerator\n    participant BT as Backtester\n    M->>DL: load_event_data()\n    DL-->>M: return events\n    M->>DL: load_tweet_data()\n    DL-->>M: return tweets\n    M->>PR: preprocess(events)\n    PR-->>M: preprocessed_events\n    M->>PR: preprocess(tweets)\n    PR-->>M: preprocessed_tweets\n    M->>ML: label_data(preprocessed_events)\n    ML-->>M: labeled_events\n    M->>MD: initialize model()\n    M->>TR: train(model, labeled_events)\n    TR->>MD: forward(x)\n    MD-->>TR: predictions\n    TR-->>M: training complete\n    M->>EV: evaluate(model, labeled_events)\n    EV->>MD: forward(x)\n    MD-->>EV: predictions\n    EV-->>M: metrics\n    M->>SG: generate_signals(preprocessed_tweets)\n    SG->>MD: forward(x)\n    MD-->>SG: predictions\n    SG-->>M: signals\n    M->>BT: run_backtest(signals)\n    BT-->>M: backtest_results\n\",\n    \"Anything UNCLEAR\": \"Need clarification on the exact dataset format, the specific implementation of the prompt-engineering-based method, and any proprietary data preprocessing steps that may have been used.\"\n}\n[/CONTENT]"}, {"role": "user", "content": "Your goal is break down tasks according to PRD/technical design, generate a task list, and analyze task dependencies. \nYou will break down tasks, analyze dependencies.\n             \nYou outline a clear PRD/technical design for reproducing the paper\u2019s method and experiments. \n\nNow, let's break down tasks according to PRD/technical design, generate a task list, and analyze task dependencies.\nThe Logic Analysis should not only consider the dependencies between files but also provide detailed descriptions to assist in writing the code needed to reproduce the paper.\n\n-----\n\n## Format Example\n[CONTENT]\n{\n    \"Required packages\": [\n        \"numpy==1.21.0\",\n        \"torch==1.9.0\"  \n    ],\n    \"Required Other language third-party packages\": [\n        \"No third-party dependencies required\"\n    ],\n    \"Logic Analysis\": [\n        [\n            \"data_preprocessing.py\",\n            \"DataPreprocessing class ........\"\n        ],\n        [\n            \"trainer.py\",\n            \"Trainer ....... \"\n        ],\n        [\n            \"dataset_loader.py\",\n            \"Handles loading and ........\"\n        ],\n        [\n            \"model.py\",\n            \"Defines the model .......\"\n        ],\n        [\n            \"evaluation.py\",\n            \"Evaluation class ........ \"\n        ],\n        [\n            \"main.py\",\n            \"Entry point  .......\"\n        ]\n    ],\n    \"Task list\": [\n        \"dataset_loader.py\", \n        \"model.py\",  \n        \"trainer.py\", \n        \"evaluation.py\",\n        \"main.py\"  \n    ],\n    \"Full API spec\": \"openapi: 3.0.0 ...\",\n    \"Shared Knowledge\": \"Both data_preprocessing.py and trainer.py share ........\",\n    \"Anything UNCLEAR\": \"Clarification needed on recommended hardware configuration for large-scale experiments.\"\n}\n\n[/CONTENT]\n\n## Nodes: \"<node>: <type>  # <instruction>\"\n- Required packages: typing.Optional[typing.List[str]]  # Provide required third-party packages in requirements.txt format.(e.g., 'numpy==1.21.0').\n- Required Other language third-party packages: typing.List[str]  # List down packages required for non-Python languages. If none, specify \"No third-party dependencies required\".\n- Logic Analysis: typing.List[typing.List[str]]  # Provide a list of files with the classes/methods/functions to be implemented, including dependency analysis and imports. Include as much detailed description as possible.\n- Task list: typing.List[str]  # Break down the tasks into a list of filenames, prioritized based on dependency order. The task list must include the previously generated file list.\n- Full API spec: <class 'str'>  # Describe all APIs using OpenAPI 3.0 spec that may be used by both frontend and backend. If front-end and back-end communication is not required, leave it blank.\n- Shared Knowledge: <class 'str'>  # Detail any shared knowledge, like common utility functions or configuration variables.\n- Anything UNCLEAR: <class 'str'>  # Mention any unresolved questions or clarifications needed from the paper or project scope.\n\n## Constraint\nFormat: output wrapped inside [CONTENT][/CONTENT] like the format example, nothing else.\n\n## Action\nFollow the node instructions above, generate your output accordingly, and ensure it follows the given format example."}, {"role": "assistant", "content": "[CONTENT]\n{\n    \"Required packages\": [\n        \"pandas==1.3.3\",\n        \"tensorflow==2.6.0\",\n        \"numpy==1.21.2\",\n        \"scikit-learn==0.24.2\",\n        \"vectorbt==0.20.7\", \n        \"shap==0.39.0\"\n    ],\n    \"Required Other language third-party packages\": [\n        \"No third-party dependencies required\"\n    ],\n    \"Logic Analysis\": [\n        [\n            \"dataset_loader.py\",\n            \"Handles loading and preprocessing of Bitcoin historical events and tweet data from specified sources (e.g., Kaggle). Prepares data for further analysis.\"\n        ],\n        [\n            \"preprocessor.py\",\n            \"DataPreprocessing class implements methods to normalize text, handle missing values, and compute technical indicators like RSI and ROC.\"\n        ],\n        [\n            \"market_labeler.py\",\n            \"MarketLabeler class defines methods to apply the Triple Barrier Labeling (TBL) strategy to assign bullish, bearish, or neutral labels.\"\n        ],\n        [\n            \"model.py\",\n            \"Defines the Model classes for CryptoBERT and FinBERT, includes prompt tuning adjustments for integrating market context.\"\n        ],\n        [\n            \"trainer.py\",\n            \"Trainer class handles the model training process, utilizing data labeled by market behaviors, includes methods for managing training epochs and validation.\"\n        ],\n        [\n            \"evaluation.py\",\n            \"Evaluation class provides tools to measure model performance using metrics such as accuracy, precision, recall, and F1-score based on trading signal prediction.\"\n        ],\n        [\n            \"signal_generator.py\",\n            \"SignalGenerator class creates trading signals using the trained model's predictions, also adjusts signals based on majority or mean methods.\"\n        ],\n        [\n            \"backtester.py\",\n            \"Backtester class uses vectorbt library to simulate trades and evaluate trading strategy performance across market conditions defined in the paper.\"\n        ],\n        [\n            \"main.py\",\n            \"Entry point for executing the whole program; coordinates data loading, preprocessing, model training, evaluation, signal generation, and backtesting.\"\n        ]\n    ],\n    \"Task list\": [\n        \"dataset_loader.py\",\n        \"preprocessor.py\",\n        \"market_labeler.py\",\n        \"model.py\",\n        \"trainer.py\",\n        \"evaluation.py\",\n        \"signal_generator.py\",\n        \"backtester.py\",\n        \"main.py\"\n    ],\n    \"Full API spec\": \"\",\n    \"Shared Knowledge\": \"Both dataset_loader.py and preprocessor.py share a config module for reading data paths and preprocessing settings. Preprocessing settings are also shared between preprocessor.py and market_labeler.py.\",\n    \"Anything UNCLEAR\": \"Clarification needed on the specific encoding configurations used in prompt tuning and any detailed parameter settings for the market labeling algorithm.\"\n}\n[/CONTENT]"}, {"role": "user", "content": "You write elegant, modular, and maintainable code. Adhere to Google-style guidelines.\n\nBased on the paper, plan, design specified previously, follow the \"Format Example\" and generate the code. \nExtract the training details from the above paper (e.g., learning rate, batch size, epochs, etc.), follow the \"Format example\" and generate the code. \nDO NOT FABRICATE DETAILS \u2014 only use what the paper provides.\n\nYou must write `config.yaml`.\n\nATTENTION: Use '##' to SPLIT SECTIONS, not '#'. Your output format must follow the example below exactly.\n\n-----\n\n# Format Example\n## Code: config.yaml\n```yaml\n## config.yaml\ntraining:\n  learning_rate: ...\n  batch_size: ...\n  epochs: ...\n...\n```\n\n-----\n\n## Code: config.yaml\n"}, {"role": "assistant", "content": "```yaml\n## config.yaml\ntraining:\n  learning_rate: 1e-5\n  batch_size: 12\n  epochs: 2\n  optimizer: AdamW\n  warmup_steps: 0.1  # 10% of total training steps\ndata:\n  bitcoin_events_path: \"path/to/bitcoin_historical_events.csv\"  # Replace with actual path\n  tweets_data_path: \"path/to/tweet_data.csv\"  # Replace with actual path\n  output_data_path: \"path/to/output\"  # Path to save processed data\n  rsi_threshold: [30, 70]\n  roc_window_length: 8  # Assuming based on text\n  preprocessing_steps:\n    - text_normalization: true\n    - remove_urls: true\n    - remove_user_ids: true\n    - remove_punctuation: true\n    - lemmatization: true\nmodel:\n  type: \"CryptoBERT\"  # Options: CryptoBERT, FinBERT\n  prompt_tuning: true\nbacktesting:\n  using_library: \"vectorbt\"\nevaluation:\n  metrics:\n    - accuracy\n    - precision\n    - recall\n    - f1_score\nsignal_generation:\n  methods:\n    - majority\n    - mean\nmarket_labeling:\n  strategy: \"TBL\"  # Triple Barrier Labeling strategy\n  barrier_window: 8-15  # Specified in paper\n```\n"}]